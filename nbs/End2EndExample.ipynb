{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import sys, importlib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from connections import SnowflakeConnection\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "from snowflake.ml.model import target_platform as snow_target_platform\n",
    "TargetPlatform = snow_target_platform.TargetPlatform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOGGER_CONFIGURED = False\n",
    "\n",
    "DEFAULT_LOG_FORMAT = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n",
    "DEFAULT_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "DEFAULT_LOG_LEVEL = logging.INFO\n",
    "ENV_VAR_LOG_LEVEL = \"MODEL_REGISTRY_LOG_LEVEL\"\n",
    "\n",
    "\n",
    "def _normalize_level(value: str, fallback: int) -> int:\n",
    "    \"\"\"Resolve a logging level string or numeric value to an int.\"\"\"\n",
    "    if value is None:\n",
    "        return fallback\n",
    "\n",
    "    if isinstance(value, str):\n",
    "        level = logging.getLevelName(value.upper())\n",
    "        if isinstance(level, int):\n",
    "            return level\n",
    "    elif isinstance(value, int):\n",
    "        return value\n",
    "\n",
    "    return fallback\n",
    "\n",
    "\n",
    "def setup_logging(\n",
    "    default_level: int = DEFAULT_LOG_LEVEL,\n",
    "    env_var: str = ENV_VAR_LOG_LEVEL,\n",
    "    force: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Configure root logging once for scripts and notebooks.\"\"\"\n",
    "    global _LOGGER_CONFIGURED\n",
    "\n",
    "    if _LOGGER_CONFIGURED and not force:\n",
    "        return\n",
    "\n",
    "    env_level = os.getenv(env_var)\n",
    "    level = _normalize_level(env_level, default_level)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "\n",
    "    if force:\n",
    "        for handler in root_logger.handlers[:]:\n",
    "            root_logger.removeHandler(handler)\n",
    "\n",
    "    if not root_logger.handlers:\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter(DEFAULT_LOG_FORMAT, DEFAULT_DATE_FORMAT))\n",
    "        root_logger.addHandler(handler)\n",
    "\n",
    "    root_logger.setLevel(level)\n",
    "    _LOGGER_CONFIGURED = True\n",
    "\n",
    "\n",
    "def get_logger(name: Optional[str] = None, level: Optional[int] = None) -> logging.Logger:\n",
    "    \"\"\"Return a configured logger for the given name.\"\"\"\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(name)\n",
    "    if level is not None:\n",
    "        logger.setLevel(level)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def log_section(\n",
    "    logger: logging.Logger,\n",
    "    title: str,\n",
    "    level: int = logging.INFO,\n",
    "    width: int = 80,\n",
    "    pad_char: str = \"=\",\n",
    ") -> None:\n",
    "    \"\"\"Log a section heading similar to the previous banner prints.\"\"\"\n",
    "    border = pad_char * width if pad_char else \"\"\n",
    "    if border:\n",
    "        logger.log(level, border)\n",
    "    logger.log(level, title)\n",
    "    if border:\n",
    "        logger.log(level, border)\n",
    "\n",
    "\n",
    "def set_global_level(level: int) -> None:\n",
    "    \"\"\"Allow callers to adjust root log level at runtime.\"\"\"\n",
    "    setup_logging(force=False)\n",
    "    logging.getLogger().setLevel(level)\n",
    "\n",
    "logger = get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    n_samples: int = 10000\n",
    "    n_features: int = 20\n",
    "    random_state: int = 42\n",
    "    csv_path: Path = Path(\"synthetic_data.csv\")\n",
    "    upload_to_snowflake: bool = True\n",
    "    connection_name: str = \"legalzoom\"\n",
    "    database: str = \"ML_SHOWCASE\"\n",
    "    data_schema: str = \"DATA\"\n",
    "    table_name: str = \"SYNTHETIC_DATA\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    scaler_path: Path = Path(\"scaler.pkl\")\n",
    "    model_path: Path = Path(\"model.pkl\")\n",
    "    test_data_path: Path = Path(\"test_data.csv\")\n",
    "    metrics_path: Path = Path(\"model_metrics.json\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegistryConfig:\n",
    "    connection_name: str = \"legalzoom\"\n",
    "    database: str = \"ML_SHOWCASE\"\n",
    "    schema: str = \"MODELS\"\n",
    "    model_name: str = \"LINEAR_REGRESSION_CUSTOM\"\n",
    "    user_files: Dict[str, list[str]] = field(default_factory=lambda: {\"preprocessing\": [\"scaler.pkl\"]})\n",
    "    conda_dependencies: list[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"numpy==1.26.4\",\n",
    "            \"pandas==2.1.4\",\n",
    "            \"scikit-learn==1.5.2\",\n",
    "        ]\n",
    "    )\n",
    "    pip_requirements: list[str] = field(default_factory=list)\n",
    "    artifact_repository_map: Optional[Dict[str, str]] = None\n",
    "    resource_constraint: Optional[Dict[str, str]] = None\n",
    "    python_version: str = \"3.10\"\n",
    "    enable_explainability: bool = False\n",
    "    target_platform_mode: str = \"WAREHOUSE_ONLY\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineSteps:\n",
    "    generate_data: bool = True\n",
    "    train_model: bool = True\n",
    "    verify_pickles: bool = True\n",
    "    log_model: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ServingConfig:\n",
    "    enabled: bool = False\n",
    "    compute_pool: Optional[str] = None\n",
    "    service_name: Optional[str] = None\n",
    "    min_instances: int = 1\n",
    "    max_instances: int = 1\n",
    "    instance_family: str = \"CPU_X64_M\"\n",
    "    force_rebuild: bool = False\n",
    "    drop_existing_service: bool = True\n",
    "    num_workers: Optional[int] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    train: TrainConfig = field(default_factory=TrainConfig)\n",
    "    registry: RegistryConfig = field(default_factory=RegistryConfig)\n",
    "    steps: PipelineSteps = field(default_factory=PipelineSteps)\n",
    "    serving: ServingConfig = field(default_factory=ServingConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cfg = {\n",
    "    \"data\": {\n",
    "        # dataset generation\n",
    "        \"n_samples\": 5000,\n",
    "        \"n_features\": 20,\n",
    "        \"random_state\": 42,\n",
    "        \"csv_path\": \"notebook_synthetic_data.csv\",\n",
    "        \"upload_to_snowflake\": True,  # flip to True when ready\n",
    "        \"connection_name\": \"legalzoom\",\n",
    "        \"database\": \"ML_SHOWCASE\",\n",
    "        \"data_schema\": \"DATA\",\n",
    "        \"table_name\": \"SYNTHETIC_DATA\",\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"test_size\": 0.2,\n",
    "        \"random_state\": 42,\n",
    "        \"scaler_path\": \"scaler.pkl\",\n",
    "        \"model_path\": \"model.pkl\",\n",
    "        \"test_data_path\": \"test_data.csv\",\n",
    "        \"metrics_path\": \"model_metrics.json\",\n",
    "    },\n",
    "    \"registry\": {\n",
    "        \"connection_name\": \"legalzoom\",\n",
    "        \"database\": \"ML_SHOWCASE\",\n",
    "        \"schema\": \"MODELS\",\n",
    "        \"model_name\": \"LINEAR_REGRESSION_CUSTOM\",\n",
    "        \"target_platform_mode\": \"SNOWPARK_CONTAINER_SERVICES_ONLY\",\n",
    "        \"conda_dependencies\": [],\n",
    "        \"pip_requirements\": [\n",
    "            \"numpy==1.26.4\",\n",
    "            \"pandas==2.1.4\",\n",
    "            \"scikit-learn==1.5.2\",\n",
    "        ],\n",
    "        # Provide these when targeting warehouses to satisfy pip installations:\n",
    "        # \"artifact_repository_map\": {\"pip\": \"snowflake.snowpark.pypi_shared_repository\"},\n",
    "        # \"resource_constraint\": {\"class\": \"STANDARD_GEN_1\"},\n",
    "    },\n",
    "    \"steps\": {\n",
    "        \"generate_data\": True,\n",
    "        \"train_model\": True,\n",
    "        \"verify_pickles\": True,\n",
    "        \"log_model\": False,  # set True once you're satisfied with the run\n",
    "    },\n",
    "    \"serving\": {\n",
    "        \"enabled\": False,\n",
    "        \"compute_pool\": \"ML_INFERENCE_POOL\",\n",
    "        \"service_name\": \"LINEAR_REGRESSION_SERVICE\",\n",
    "        \"min_instances\": 1,\n",
    "        \"max_instances\": 1,\n",
    "        \"instance_family\": \"CPU_X64_M\",\n",
    "        \"force_rebuild\": True,\n",
    "        \"drop_existing_service\": True,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_config_from_mapping(mapping: Dict[str, Any]) -> PipelineConfig:\n",
    "    \"\"\"Create a pipeline configuration from a nested mapping.\"\"\"\n",
    "    cfg = PipelineConfig()\n",
    "\n",
    "    def _apply(target: Any, values: Dict[str, Any]) -> None:\n",
    "        for key, value in values.items():\n",
    "            if hasattr(target, key):\n",
    "                current = getattr(target, key)\n",
    "                if isinstance(current, Path) and not isinstance(value, Path):\n",
    "                    setattr(target, key, Path(value))\n",
    "                else:\n",
    "                    setattr(target, key, value)\n",
    "\n",
    "    if \"data\" in mapping:\n",
    "        _apply(cfg.data, mapping[\"data\"])\n",
    "    if \"train\" in mapping:\n",
    "        _apply(cfg.train, mapping[\"train\"])\n",
    "    if \"registry\" in mapping:\n",
    "        _apply(cfg.registry, mapping[\"registry\"])\n",
    "    if \"steps\" in mapping:\n",
    "        _apply(cfg.steps, mapping[\"steps\"])\n",
    "    if \"serving\" in mapping:\n",
    "        _apply(cfg.serving, mapping[\"serving\"])\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineConfig(data=DataConfig(n_samples=5000, n_features=20, random_state=42, csv_path=PosixPath('notebook_synthetic_data.csv'), upload_to_snowflake=True, connection_name='legalzoom', database='ML_SHOWCASE', data_schema='DATA', table_name='SYNTHETIC_DATA'), train=TrainConfig(test_size=0.2, random_state=42, scaler_path=PosixPath('scaler.pkl'), model_path=PosixPath('model.pkl'), test_data_path=PosixPath('test_data.csv'), metrics_path=PosixPath('model_metrics.json')), registry=RegistryConfig(connection_name='legalzoom', database='ML_SHOWCASE', schema='MODELS', model_name='LINEAR_REGRESSION_CUSTOM', user_files={'preprocessing': ['scaler.pkl']}, conda_dependencies=[], pip_requirements=['numpy==1.26.4', 'pandas==2.1.4', 'scikit-learn==1.5.2'], artifact_repository_map=None, resource_constraint=None, python_version='3.10', enable_explainability=False, target_platform_mode='SNOWPARK_CONTAINER_SERVICES_ONLY'), steps=PipelineSteps(generate_data=True, train_model=True, verify_pickles=True, log_model=False), serving=ServingConfig(enabled=False, compute_pool='ML_INFERENCE_POOL', service_name='LINEAR_REGRESSION_SERVICE', min_instances=1, max_instances=1, instance_family='CPU_X64_M', force_rebuild=True, drop_existing_service=True, num_workers=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_cfg = pipeline_config_from_mapping(base_cfg)\n",
    "pipeline_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 17:15:20 | INFO | snowflake.connector.connection | Snowflake Connector for Python Version: 3.18.0, Python Version: 3.10.19, Platform: macOS-15.7.2-x86_64-i386-64bit\n",
      "2025-11-11 17:15:20 | INFO | snowflake.connector.connection | Connecting to GLOBAL Snowflake domain\n",
      "2025-11-11 17:15:21 | INFO | snowflake.snowpark.session | Snowpark Session information: \n",
      "\"version\" : 1.42.0,\n",
      "\"python.version\" : 3.10.19,\n",
      "\"python.connector.version\" : 3.18.0,\n",
      "\"python.connector.session.id\" : 21917694968801730,\n",
      "\"os.name\" : Darwin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    connection = SnowflakeConnection.from_snow_cli(pipeline_cfg.data.connection_name)\n",
    "    session = connection.session\n",
    "except ImportError:\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(config: DataConfig) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic regression data.\"\"\"\n",
    "    log_section(logger, \"GENERATING SYNTHETIC DATASET\")\n",
    "\n",
    "    from sklearn.datasets import make_regression\n",
    "\n",
    "    X, y = make_regression(\n",
    "        n_samples=config.n_samples,\n",
    "        n_features=config.n_features,\n",
    "        n_informative=min(config.n_features, 15),\n",
    "        n_targets=1,\n",
    "        noise=10.0,\n",
    "        bias=50.0,\n",
    "        random_state=config.random_state,\n",
    "    )\n",
    "\n",
    "    feature_names = [f\"FEATURE_{i:02d}\" for i in range(config.n_features)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df[\"TARGET\"] = y\n",
    "    df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "\n",
    "    logger.info(\"Dataset summary: samples=%s, features=%s\", f\"{config.n_samples:,}\", config.n_features)\n",
    "    logger.info(\"Target mean=%.2f std=%.2f\", df[\"TARGET\"].mean(), df[\"TARGET\"].std())\n",
    "    return df\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, path: Path) -> Path:\n",
    "    \"\"\"Persist dataframe to CSV.\"\"\"\n",
    "    log_section(logger, \"SAVING DATA TO CSV\")\n",
    "    df.to_csv(path, index=False)\n",
    "    logger.info(\"Saved data to %s (%.2f MB)\", path, path.stat().st_size / (1024 * 1024))\n",
    "    return path\n",
    "\n",
    "def upload_to_snowflake(df: pd.DataFrame, config: DataConfig) -> Optional[str]:\n",
    "    \"\"\"Upload dataframe to Snowflake if requested.\"\"\"\n",
    "    if not config.upload_to_snowflake:\n",
    "        logger.info(\"Snowflake upload skipped (upload_to_snowflake=False)\")\n",
    "        return None\n",
    "\n",
    "    log_section(logger, \"UPLOADING DATA TO SNOWFLAKE\")\n",
    "    try:\n",
    "        connection = SnowflakeConnection.from_snow_cli(config.connection_name)\n",
    "        session = connection.session\n",
    "    except ImportError:\n",
    "        from snowflake.snowpark.context import get_active_session\n",
    "        session = get_active_session()\n",
    "    try:\n",
    "        session.sql(f\"CREATE DATABASE IF NOT EXISTS {config.database}\").collect()\n",
    "        session.sql(f\"CREATE SCHEMA IF NOT EXISTS {config.data_schema}\").collect()\n",
    "        session.sql(f\"USE DATABASE {config.database}\").collect()\n",
    "        session.sql(f\"USE SCHEMA {config.data_schema}\").collect()\n",
    "\n",
    "        session.create_dataframe(df).write.mode(\"overwrite\").save_as_table(config.table_name)\n",
    "        logger.info(\n",
    "            \"Uploaded data to %s.%s.%s\", config.database, config.data_schema, config.table_name\n",
    "        )\n",
    "        return f\"{config.database}.{config.data_schema}.{config.table_name}\"\n",
    "    finally:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataConfig(n_samples=5000, n_features=20, random_state=42, csv_path=PosixPath('notebook_synthetic_data.csv'), upload_to_snowflake=True, connection_name='legalzoom', database='ML_SHOWCASE', data_schema='DATA', table_name='SYNTHETIC_DATA')\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 17:15:28 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:15:28 | INFO | __main__ | GENERATING SYNTHETIC DATASET\n",
      "2025-11-11 17:15:28 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:15:28 | INFO | __main__ | Dataset summary: samples=5,000, features=20\n",
      "2025-11-11 17:15:28 | INFO | __main__ | Target mean=49.57 std=210.02\n",
      "2025-11-11 17:15:28 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:15:28 | INFO | __main__ | SAVING DATA TO CSV\n",
      "2025-11-11 17:15:28 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:15:28 | INFO | __main__ | Saved data to notebook_synthetic_data.csv (1.98 MB)\n",
      "2025-11-11 17:15:28 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:15:28 | INFO | __main__ | UPLOADING DATA TO SNOWFLAKE\n",
      "2025-11-11 17:15:28 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:15:29 | INFO | snowflake.connector.connection | Snowflake Connector for Python Version: 3.18.0, Python Version: 3.10.19, Platform: macOS-15.7.2-x86_64-i386-64bit\n",
      "2025-11-11 17:15:29 | INFO | snowflake.connector.connection | Connecting to GLOBAL Snowflake domain\n",
      "2025-11-11 17:15:29 | INFO | snowflake.snowpark.session | Snowpark Session information: \n",
      "\"version\" : 1.42.0,\n",
      "\"python.version\" : 3.10.19,\n",
      "\"python.connector.version\" : 3.18.0,\n",
      "\"python.connector.session.id\" : 21917694968805346,\n",
      "\"os.name\" : Darwin\n",
      "\n",
      "2025-11-11 17:15:37 | INFO | __main__ | Uploaded data to ML_SHOWCASE.DATA.SYNTHETIC_DATA\n",
      "2025-11-11 17:15:37 | INFO | snowflake.snowpark.session | Closing session: 21917694968805346\n",
      "2025-11-11 17:15:37 | INFO | snowflake.snowpark.session | Canceling all running queries\n",
      "2025-11-11 17:15:37 | INFO | snowflake.snowpark.session | Closed session: 21917694968805346\n"
     ]
    }
   ],
   "source": [
    "outputs = {}\n",
    "\n",
    "df = generate_synthetic_data(pipeline_cfg.data)\n",
    "csv_path = save_to_csv(df, pipeline_cfg.data.csv_path)\n",
    "table_name = upload_to_snowflake(df, pipeline_cfg.data)\n",
    "\n",
    "outputs[\"dataframe\"] = df\n",
    "outputs[\"csv_path\"] = csv_path\n",
    "outputs[\"table_name\"] = table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FEATURE_00</th>\n",
       "      <th>FEATURE_01</th>\n",
       "      <th>FEATURE_02</th>\n",
       "      <th>FEATURE_03</th>\n",
       "      <th>FEATURE_04</th>\n",
       "      <th>FEATURE_05</th>\n",
       "      <th>FEATURE_06</th>\n",
       "      <th>FEATURE_07</th>\n",
       "      <th>FEATURE_08</th>\n",
       "      <th>...</th>\n",
       "      <th>FEATURE_11</th>\n",
       "      <th>FEATURE_12</th>\n",
       "      <th>FEATURE_13</th>\n",
       "      <th>FEATURE_14</th>\n",
       "      <th>FEATURE_15</th>\n",
       "      <th>FEATURE_16</th>\n",
       "      <th>FEATURE_17</th>\n",
       "      <th>FEATURE_18</th>\n",
       "      <th>FEATURE_19</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.024612</td>\n",
       "      <td>0.232092</td>\n",
       "      <td>-2.465177</td>\n",
       "      <td>-0.866399</td>\n",
       "      <td>-1.213830</td>\n",
       "      <td>-2.229094</td>\n",
       "      <td>-0.253414</td>\n",
       "      <td>-0.202641</td>\n",
       "      <td>-1.506257</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.206291</td>\n",
       "      <td>0.503526</td>\n",
       "      <td>0.654738</td>\n",
       "      <td>0.383037</td>\n",
       "      <td>-0.046785</td>\n",
       "      <td>1.317545</td>\n",
       "      <td>0.876958</td>\n",
       "      <td>0.453318</td>\n",
       "      <td>-0.389777</td>\n",
       "      <td>-240.944289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.109940</td>\n",
       "      <td>1.517878</td>\n",
       "      <td>-0.177179</td>\n",
       "      <td>0.605805</td>\n",
       "      <td>-0.813199</td>\n",
       "      <td>0.152118</td>\n",
       "      <td>-0.048728</td>\n",
       "      <td>0.035221</td>\n",
       "      <td>0.163817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555265</td>\n",
       "      <td>-3.419906</td>\n",
       "      <td>-0.520985</td>\n",
       "      <td>-1.521669</td>\n",
       "      <td>0.377092</td>\n",
       "      <td>-1.384809</td>\n",
       "      <td>1.153584</td>\n",
       "      <td>0.692579</td>\n",
       "      <td>0.520902</td>\n",
       "      <td>158.905363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.439888</td>\n",
       "      <td>1.628730</td>\n",
       "      <td>0.059572</td>\n",
       "      <td>-0.602144</td>\n",
       "      <td>1.220722</td>\n",
       "      <td>0.029977</td>\n",
       "      <td>-1.037918</td>\n",
       "      <td>-0.616253</td>\n",
       "      <td>-0.226677</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.900659</td>\n",
       "      <td>0.868561</td>\n",
       "      <td>-1.054836</td>\n",
       "      <td>0.808803</td>\n",
       "      <td>-0.716726</td>\n",
       "      <td>0.310161</td>\n",
       "      <td>0.200068</td>\n",
       "      <td>-0.620061</td>\n",
       "      <td>0.932003</td>\n",
       "      <td>-0.705998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.322548</td>\n",
       "      <td>1.623844</td>\n",
       "      <td>-0.859438</td>\n",
       "      <td>-2.094438</td>\n",
       "      <td>-0.431001</td>\n",
       "      <td>-1.487290</td>\n",
       "      <td>-1.950900</td>\n",
       "      <td>-0.050942</td>\n",
       "      <td>1.416834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131180</td>\n",
       "      <td>-0.675938</td>\n",
       "      <td>-0.226175</td>\n",
       "      <td>-0.046749</td>\n",
       "      <td>-0.399696</td>\n",
       "      <td>-2.911804</td>\n",
       "      <td>0.774936</td>\n",
       "      <td>0.092381</td>\n",
       "      <td>-0.799743</td>\n",
       "      <td>-85.849133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.830849</td>\n",
       "      <td>2.508861</td>\n",
       "      <td>-0.834636</td>\n",
       "      <td>-1.580244</td>\n",
       "      <td>0.030498</td>\n",
       "      <td>0.939570</td>\n",
       "      <td>0.719350</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>-0.118382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089481</td>\n",
       "      <td>-0.146882</td>\n",
       "      <td>0.928356</td>\n",
       "      <td>0.382645</td>\n",
       "      <td>-0.109746</td>\n",
       "      <td>1.383421</td>\n",
       "      <td>1.124860</td>\n",
       "      <td>0.990663</td>\n",
       "      <td>-0.296109</td>\n",
       "      <td>186.936427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  FEATURE_00  FEATURE_01  FEATURE_02  FEATURE_03  FEATURE_04  FEATURE_05  \\\n",
       "0   1    2.024612    0.232092   -2.465177   -0.866399   -1.213830   -2.229094   \n",
       "1   2   -0.109940    1.517878   -0.177179    0.605805   -0.813199    0.152118   \n",
       "2   3   -0.439888    1.628730    0.059572   -0.602144    1.220722    0.029977   \n",
       "3   4    1.322548    1.623844   -0.859438   -2.094438   -0.431001   -1.487290   \n",
       "4   5    0.830849    2.508861   -0.834636   -1.580244    0.030498    0.939570   \n",
       "\n",
       "   FEATURE_06  FEATURE_07  FEATURE_08  ...  FEATURE_11  FEATURE_12  \\\n",
       "0   -0.253414   -0.202641   -1.506257  ...   -1.206291    0.503526   \n",
       "1   -0.048728    0.035221    0.163817  ...    0.555265   -3.419906   \n",
       "2   -1.037918   -0.616253   -0.226677  ...   -1.900659    0.868561   \n",
       "3   -1.950900   -0.050942    1.416834  ...   -0.131180   -0.675938   \n",
       "4    0.719350   -0.008313   -0.118382  ...   -0.089481   -0.146882   \n",
       "\n",
       "   FEATURE_13  FEATURE_14  FEATURE_15  FEATURE_16  FEATURE_17  FEATURE_18  \\\n",
       "0    0.654738    0.383037   -0.046785    1.317545    0.876958    0.453318   \n",
       "1   -0.520985   -1.521669    0.377092   -1.384809    1.153584    0.692579   \n",
       "2   -1.054836    0.808803   -0.716726    0.310161    0.200068   -0.620061   \n",
       "3   -0.226175   -0.046749   -0.399696   -2.911804    0.774936    0.092381   \n",
       "4    0.928356    0.382645   -0.109746    1.383421    1.124860    0.990663   \n",
       "\n",
       "   FEATURE_19      TARGET  \n",
       "0   -0.389777 -240.944289  \n",
       "1    0.520902  158.905363  \n",
       "2    0.932003   -0.705998  \n",
       "3   -0.799743  -85.849133  \n",
       "4   -0.296109  186.936427  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"SELECT * FROM ML_SHOWCASE.DATA.SYNTHETIC_DATA LIMIT 5\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Z-Scaler & Training and Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from typing import Dict, Any\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(\n",
    "    df: pd.DataFrame, config: TrainConfig\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Split dataset into train/test folds.\"\"\"\n",
    "    features = [col for col in df.columns if col.startswith(\"FEATURE_\")]\n",
    "    X = df[features]\n",
    "    y = df[\"TARGET\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=config.test_size,\n",
    "        random_state=config.random_state,\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Split data: train=%s, test=%s (test_size=%.0f%%)\",\n",
    "        f\"{len(X_train):,}\",\n",
    "        f\"{len(X_test):,}\",\n",
    "        config.test_size * 100,\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_model import CustomZScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_with_preprocessing(X_train: pd.DataFrame, y_train: pd.Series):\n",
    "    \"\"\"\n",
    "    Train model with preprocessing and save separate pickle files.\n",
    "    \n",
    "    This function:\n",
    "    1. Creates and fits the custom Z-scaler\n",
    "    2. Transforms training data\n",
    "    3. Trains linear regression model\n",
    "    4. Saves scaler and model as separate pickle files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.Series or np.ndarray\n",
    "        Training targets\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    scaler : CustomZScaler\n",
    "        Fitted scaler\n",
    "    model : LinearRegression\n",
    "        Fitted model\n",
    "    \"\"\"\n",
    "    log_section(logger, \"TRAINING MODEL WITH PREPROCESSING\")\n",
    "    \n",
    "    # Step 1: Create and fit the custom Z-scaler\n",
    "    logger.info(\"1. Fitting Custom Z-Scaler...\")\n",
    "    scaler = CustomZScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    logger.info(\"   Scaler fitted\")\n",
    "    logger.info(\"   Features: %s\", len(scaler.feature_names_))\n",
    "    logger.info(\n",
    "        \"   Mean range: [%.2f, %.2f]\",\n",
    "        scaler.mean_.min(),\n",
    "        scaler.mean_.max(),\n",
    "    )\n",
    "    logger.info(\n",
    "        \"   Std range: [%.2f, %.2f]\",\n",
    "        scaler.std_.min(),\n",
    "        scaler.std_.max(),\n",
    "    )\n",
    "    \n",
    "    # Step 2: Train linear regression model\n",
    "    logger.info(\"2. Training Linear Regression Model...\")\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    logger.info(\"   Model trained\")\n",
    "    logger.info(\"   Coefficients: %s\", len(model.coef_))\n",
    "    logger.info(\"   Intercept: %.2f\", model.intercept_)\n",
    "    \n",
    "    # Step 3: Save as separate pickle files\n",
    "    logger.info(\"3. Saving Components as Separate Pickle Files...\")\n",
    "    \n",
    "    scaler_path = \"scaler.pkl\"\n",
    "    model_path = \"model.pkl\"\n",
    "    \n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    logger.info(\"   Saved scaler to: %s\", scaler_path)\n",
    "    \n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    logger.info(\"   Saved model to: %s\", model_path)\n",
    "    \n",
    "    logger.info(\"Training complete!\")\n",
    "    logger.info(\"  Components saved as separate pickle files:\")\n",
    "    logger.info(\"    - %s (preprocessing)\", scaler_path)\n",
    "    logger.info(\"    - %s (model)\", model_path)\n",
    "    \n",
    "    return scaler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    scaler,\n",
    "    model,\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Compute evaluation metrics for train/test sets.\"\"\"\n",
    "    log_section(logger, \"EVALUATING MODEL\")\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    metrics = {\n",
    "        \"train\": {\n",
    "            \"mse\": mean_squared_error(y_train, train_pred),\n",
    "            \"rmse\": np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            \"mae\": mean_absolute_error(y_train, train_pred),\n",
    "            \"r2\": r2_score(y_train, train_pred),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"mse\": mean_squared_error(y_test, test_pred),\n",
    "            \"rmse\": np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            \"mae\": mean_absolute_error(y_test, test_pred),\n",
    "            \"r2\": r2_score(y_test, test_pred),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        logger.info(\n",
    "            \"%s metrics: RMSE=%.4f MAE=%.4f R2=%.4f\",\n",
    "            split.capitalize(),\n",
    "            metrics[split][\"rmse\"],\n",
    "            metrics[split][\"mae\"],\n",
    "            metrics[split][\"r2\"],\n",
    "        )\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(test_size=0.2, random_state=42, scaler_path=PosixPath('scaler.pkl'), model_path=PosixPath('model.pkl'), test_data_path=PosixPath('test_data.csv'), metrics_path=PosixPath('model_metrics.json'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_cfg.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 17:17:24 | INFO | __main__ | Split data: train=4,000, test=1,000 (test_size=20%)\n",
      "2025-11-11 17:17:24 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:17:24 | INFO | __main__ | TRAINING MODEL WITH PREPROCESSING\n",
      "2025-11-11 17:17:24 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:17:24 | INFO | __main__ | 1. Fitting Custom Z-Scaler...\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Scaler fitted\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Features: 20\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Mean range: [-0.03, 0.03]\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Std range: [0.98, 1.03]\n",
      "2025-11-11 17:17:24 | INFO | __main__ | 2. Training Linear Regression Model...\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Model trained\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Coefficients: 20\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Intercept: 49.84\n",
      "2025-11-11 17:17:24 | INFO | __main__ | 3. Saving Components as Separate Pickle Files...\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Saved scaler to: scaler.pkl\n",
      "2025-11-11 17:17:24 | INFO | __main__ |    Saved model to: model.pkl\n",
      "2025-11-11 17:17:24 | INFO | __main__ | Training complete!\n",
      "2025-11-11 17:17:24 | INFO | __main__ |   Components saved as separate pickle files:\n",
      "2025-11-11 17:17:24 | INFO | __main__ |     - scaler.pkl (preprocessing)\n",
      "2025-11-11 17:17:24 | INFO | __main__ |     - model.pkl (model)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_training_data(df, pipeline_cfg.train)\n",
    "scaler, model = train_model_with_preprocessing(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl\n",
      "scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls | grep pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 17:17:30 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:17:30 | INFO | __main__ | EVALUATING MODEL\n",
      "2025-11-11 17:17:30 | INFO | __main__ | ================================================================================\n",
      "2025-11-11 17:17:30 | INFO | __main__ | Train metrics: RMSE=9.9837 MAE=7.8813 R2=0.9978\n",
      "2025-11-11 17:17:30 | INFO | __main__ | Test metrics: RMSE=10.1775 MAE=8.1768 R2=0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': {'mse': 99.67352813300454,\n",
       "  'rmse': np.float64(9.983663061872859),\n",
       "  'mae': 7.881317338310466,\n",
       "  'r2': 0.9977512123073183},\n",
       " 'test': {'mse': 103.582271504573,\n",
       "  'rmse': np.float64(10.177537595340683),\n",
       "  'mae': 8.176791003990894,\n",
       "  'r2': 0.9976023228619049}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = evaluate_model(scaler, model, X_train, X_test, y_train, y_test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model import custom_model\n",
    "\n",
    "model_ctx = custom_model.ModelContext(\n",
    "    model_path=str(pipeline_cfg.train.model_path.resolve()),\n",
    "    scaler_path=str(pipeline_cfg.train.scaler_path.resolve()),\n",
    ")\n",
    "\n",
    "import importlib\n",
    "import types\n",
    "\n",
    "def _ensure_custom_module_alias() -> None:\n",
    "    if \"custom_model\" in sys.modules:\n",
    "        sys.modules.setdefault(\"model_registry_showcase.custom_model\", sys.modules[\"custom_model\"])\n",
    "        return\n",
    "    module = None\n",
    "    try:\n",
    "        module = importlib.import_module(\"custom_model\")\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            module = importlib.import_module(\"model_registry_showcase.custom_model\")\n",
    "        except ModuleNotFoundError:\n",
    "            module = _create_embedded_custom_module()\n",
    "    sys.modules[\"custom_model\"] = module\n",
    "    sys.modules.setdefault(\"model_registry_showcase.custom_model\", module)\n",
    "\n",
    "def _create_embedded_custom_module() -> types.ModuleType:\n",
    "    module = types.ModuleType(\"custom_model\")\n",
    "    module.CustomZScaler = CustomZScaler\n",
    "    return module\n",
    "\n",
    "class RegressionWithScaler(custom_model.CustomModel):\n",
    "    def __init__(self, context):\n",
    "        super().__init__(context)\n",
    "        _ensure_custom_module_alias()\n",
    "        with Path(self.context[\"model_path\"]).open(\"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "        with Path(self.context[\"scaler_path\"]).open(\"rb\") as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "\n",
    "    @custom_model.inference_api\n",
    "    def predict(self, input: pd.DataFrame) -> pd.DataFrame:\n",
    "        feature_cols = [col for col in input.columns if col.startswith(\"FEATURE_\")]\n",
    "        X_scaled = self.scaler.transform(input[feature_cols])\n",
    "        y_hat = self.model.predict(X_scaled)\n",
    "        return pd.DataFrame({\"PREDICTION\": y_hat}, index=input.index)\n",
    "\n",
    "\n",
    "custom_model_obj = RegressionWithScaler(model_ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(path: Path):\n",
    "    \"\"\"Utility to load a pickle file.\"\"\"\n",
    "    with path.open(\"rb\") as fh:\n",
    "        return pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_section(logger, \"INITIALIZING SNOWFLAKE REGISTRY\")\n",
    "registry = Registry(\n",
    "    session=session,\n",
    "    database_name=pipeline_cfg.registry.database,\n",
    "    schema_name=pipeline_cfg.registry.schema,\n",
    ")\n",
    "logger.info(\"Registry ready at %s\", registry.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(pipeline_cfg.train.test_data_path)\n",
    "feature_cols = [col for col in sample_df.columns if col.startswith(\"FEATURE_\")]\n",
    "sample_data = sample_df[feature_cols].head(5)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_section(logger, \"LOGGING MODEL VERSION\")\n",
    "model = load_pickle(Path(\"model.pkl\"))\n",
    "scaler = load_pickle(Path(\"scaler.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdir, files in pipeline_cfg.registry.user_files.items():\n",
    "    for file_name in files:\n",
    "        path = Path(file_name)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"user_files entry not found: {path}\")\n",
    "        if path.stat().st_size > 5 * 1024 * 1024 * 1024:\n",
    "            raise ValueError(f\"user_files entry exceeds 5GB limit: {path}\")\n",
    "\n",
    "sample_scaled = pd.DataFrame(\n",
    "    scaler.transform(sample_data),\n",
    "    columns=sample_data.columns,\n",
    ")\n",
    "version_name = f\"v_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "logger.info(\"Logging model %s version %s\", pipeline_cfg.registry.model_name, version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_token = (pipeline_cfg.registry.target_platform_mode or \"\").upper()\n",
    "runs_in_warehouse = \"WAREHOUSE\" in mode_token if mode_token else True\n",
    "\n",
    "import importlib, sys\n",
    "custom_model_mod = importlib.import_module(\"custom_model\")\n",
    "sys.modules.setdefault(\"custom_model\", custom_model_mod)\n",
    "sys.modules.setdefault(\"model_registry_showcase.custom_model\", custom_model_mod)\n",
    "\n",
    "options = {\n",
    "    \"enable_explainability\": pipeline_cfg.registry.enable_explainability,\n",
    "    \"target_methods\": [\"predict\"],\n",
    "}\n",
    "if not pipeline_cfg.registry.pip_requirements and not pipeline_cfg.registry.conda_dependencies:\n",
    "    options[\"relax_version\"] = True\n",
    "\n",
    "log_kwargs = dict(\n",
    "    model=custom_model_obj,\n",
    "    model_name=pipeline_cfg.registry.model_name,\n",
    "    version_name=version_name,\n",
    "    comment=(\n",
    "        \"Linear Regression with custom preprocessing. \"\n",
    "        f\"Logged on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        \".\"\n",
    "    ),\n",
    "    metrics=metrics,\n",
    "    python_version=pipeline_cfg.registry.python_version,\n",
    "    sample_input_data=sample_scaled,\n",
    "    options=options,\n",
    "    ext_modules=[\n",
    "        sys.modules[RegressionWithScaler.__module__],\n",
    "        custom_model_mod,\n",
    "    ],\n",
    ")\n",
    "\n",
    "if pipeline_cfg.registry.conda_dependencies:\n",
    "    log_kwargs[\"conda_dependencies\"] = pipeline_cfg.registry.conda_dependencies\n",
    "\n",
    "if pipeline_cfg.registry.pip_requirements:\n",
    "    log_kwargs[\"pip_requirements\"] = pipeline_cfg.registry.pip_requirements\n",
    "    log_kwargs[\"options\"].pop(\"relax_version\", None)\n",
    "\n",
    "artifact_map = pipeline_cfg.registry.artifact_repository_map\n",
    "if artifact_map and not runs_in_warehouse:\n",
    "    logger.debug(\n",
    "        \"Ignoring artifact_repository_map because target platform mode %s does not include WAREHOUSE.\",\n",
    "        pipeline_cfg.registry.target_platform_mode,\n",
    "    )\n",
    "    artifact_map = None\n",
    "if artifact_map is None and pipeline_cfg.registry.pip_requirements and runs_in_warehouse:\n",
    "    artifact_map = {\"pip\": \"snowflake.snowpark.pypi_shared_repository\"}\n",
    "if artifact_map:\n",
    "    log_kwargs[\"artifact_repository_map\"] = artifact_map\n",
    "\n",
    "resource_constraint = pipeline_cfg.registry.resource_constraint if runs_in_warehouse else None\n",
    "if resource_constraint:\n",
    "    log_kwargs[\"resource_constraint\"] = resource_constraint\n",
    "\n",
    "target_platform_arg = None\n",
    "if TargetPlatform is not None and pipeline_cfg.registry.target_platform_mode:\n",
    "    target_platform_arg = getattr(TargetPlatform, pipeline_cfg.registry.target_platform_mode, None)\n",
    "    if target_platform_arg is None:\n",
    "        logger.warning(\n",
    "            \"Unknown target_platform_mode '%s'\", pipeline_cfg.registry.target_platform_mode\n",
    "        )\n",
    "elif pipeline_cfg.registry.target_platform_mode:\n",
    "    target_platform_arg = pipeline_cfg.registry.target_platform_mode\n",
    "\n",
    "if target_platform_arg:\n",
    "    log_kwargs[\"target_platform\"] = target_platform_arg\n",
    "\n",
    "model_version = registry.log_model(**log_kwargs)\n",
    "logger.info(\"Logged model version: %s\", model_version.version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"SHOW MODELS IN DATABASE ML_SHOWCASE\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import registry\n",
    "\n",
    "try:\n",
    "    reg = registry.Registry(session=session, database_name='ML_SHOWCASE', schema_name='MODELS')\n",
    "    mv = reg.get_model('LINEAR_REGRESSION_CUSTOM').version('V_20251110_190603')\n",
    "    mv.run(sample_data, function_name='PREDICT') # Need to Deploy a SPCS Endpoint\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Need to Deploy a SPCS Endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Service "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_compute_pool(session, serving_cfg: ServingConfig) -> None:\n",
    "    if not serving_cfg.compute_pool:\n",
    "        return\n",
    "\n",
    "    create_sql = f\"\"\"\n",
    "    CREATE COMPUTE POOL IF NOT EXISTS {serving_cfg.compute_pool}\n",
    "        MIN_NODES = {serving_cfg.min_instances}\n",
    "        MAX_NODES = {serving_cfg.max_instances}\n",
    "        INSTANCE_FAMILY = '{serving_cfg.instance_family}'\n",
    "    \"\"\"\n",
    "    session.sql(create_sql).collect()\n",
    "    logger.info(\"Compute pool ensured: %s\", serving_cfg.compute_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import registry\n",
    "registry = registry.Registry(session=session, database_name='ML_SHOWCASE', schema_name='MODELS')\n",
    "ensure_compute_pool(session,  pipeline_cfg.serving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_kwargs: Dict[str, Any] = {}\n",
    "compute_pool = pipeline_cfg.serving.compute_pool\n",
    "if compute_pool:\n",
    "    deploy_kwargs[\"compute_pool\"] = compute_pool\n",
    "\n",
    "base_service_name = (\n",
    "    pipeline_cfg.serving.service_name\n",
    "    or f\"{model_version.model_name}_{model_version.version_name}\"\n",
    ")\n",
    "if \".\" in base_service_name:\n",
    "    qualified_service_name = base_service_name\n",
    "else:\n",
    "    qualified_service_name = (\n",
    "        f\"{pipeline_cfg.registry.database}.{pipeline_cfg.registry.schema}.{base_service_name}\"\n",
    "    )\n",
    "\n",
    "deploy_kwargs[\"service_name\"] = qualified_service_name\n",
    "if hasattr(pipeline_cfg.serving, \"min_instances\"):\n",
    "    deploy_kwargs[\"min_instances\"] = pipeline_cfg.serving.min_instances\n",
    "if hasattr(pipeline_cfg.serving, \"max_instances\"):\n",
    "    deploy_kwargs[\"max_instances\"] = pipeline_cfg.serving.max_instances\n",
    "if getattr(pipeline_cfg.serving, \"force_rebuild\", False):\n",
    "    deploy_kwargs[\"force_rebuild\"] = pipeline_cfg.serving.force_rebuild\n",
    "if getattr(pipeline_cfg.serving, \"num_workers\", None) is not None:\n",
    "    deploy_kwargs[\"num_workers\"] = pipeline_cfg.serving.num_workers\n",
    "\n",
    "if pipeline_cfg.serving.drop_existing_service:\n",
    "    try:\n",
    "        session.sql(f\"DROP SERVICE IF EXISTS {qualified_service_name}\").collect()\n",
    "        logger.info(\"Dropped existing service if it existed: %s\", qualified_service_name)\n",
    "    except Exception as exc:\n",
    "        logger.debug(\"Unable to drop existing service %s: %s\", qualified_service_name, exc)\n",
    "\n",
    "service = None\n",
    "if hasattr(model_version, \"deploy_to_snowpark_container_services\"):\n",
    "    service = model_version.deploy_to_snowpark_container_services(**deploy_kwargs)\n",
    "elif hasattr(model_version, \"deploy\"):\n",
    "    deploy_kwargs.setdefault(\"target_platform\", \"SNOWPARK_CONTAINER_SERVICES\")\n",
    "    service = model_version.deploy(**deploy_kwargs)\n",
    "elif hasattr(model_version, \"create_service\"):\n",
    "    if not compute_pool:\n",
    "        raise ValueError(\"serving.compute_pool must be set to use create_service().\")\n",
    "    create_kwargs: Dict[str, Any] = {\n",
    "        \"service_name\": qualified_service_name,\n",
    "        \"service_compute_pool\": compute_pool,\n",
    "        \"ingress_enabled\": True,\n",
    "        \"max_instances\": pipeline_cfg.serving.max_instances,\n",
    "    }\n",
    "    if getattr(pipeline_cfg.serving, \"force_rebuild\", False):\n",
    "        create_kwargs[\"force_rebuild\"] = pipeline_cfg.serving.force_rebuild\n",
    "    if getattr(pipeline_cfg.serving, \"num_workers\", None) is not None:\n",
    "        create_kwargs[\"num_workers\"] = pipeline_cfg.serving.num_workers\n",
    "    service = model_version.create_service(**create_kwargs)\n",
    "else:\n",
    "    raise AttributeError(\"Snowflake ML SDK does not expose an SPCS deployment helper in this version.\")\n",
    "\n",
    "logger.info(\"Requested service deployment for %s\", qualified_service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# reuse the same registry/session you used to create the service\n",
    "registry = Registry(session=session, database_name=\"ML_SHOWCASE\", schema_name=\"MODELS\")\n",
    "mv = registry.get_model(\"LINEAR_REGRESSION_CUSTOM\").default   # or pick a specific version\n",
    "\n",
    "mv.show_functions()  # optional: see which inference functions are exposed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = mv.run(\n",
    "    sample_data,                      # pandas or Snowpark DataFrame\n",
    "    function_name=\"predict\",      # matches @custom_model.inference_api name\n",
    "    service_name=\"LINEAR_REGRESSION_SERVICE\"  # the service you created\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-registry-showcase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
