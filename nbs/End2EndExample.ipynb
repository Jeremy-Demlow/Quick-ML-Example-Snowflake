{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Imports ",
      "id": "a54f12c0-0ee0-44fd-bd4d-51253d0f1ad0"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": "import json\nimport pickle\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport logging\nimport os\nimport sys, importlib\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom connections import SnowflakeConnection\nfrom snowflake.snowpark.context import get_active_session\n\nfrom snowflake.ml.registry import Registry\n\nfrom snowflake.ml.model import target_platform as snow_target_platform\nTargetPlatform = snow_target_platform.TargetPlatform",
      "id": "1bb33a18-09fb-4dc9-ac8a-7f4562270e96"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": "_LOGGER_CONFIGURED = False\n\nDEFAULT_LOG_FORMAT = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\nDEFAULT_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nDEFAULT_LOG_LEVEL = logging.INFO\nENV_VAR_LOG_LEVEL = \"MODEL_REGISTRY_LOG_LEVEL\"\n\n\ndef _normalize_level(value: str, fallback: int) -> int:\n    \"\"\"Resolve a logging level string or numeric value to an int.\"\"\"\n    if value is None:\n        return fallback\n\n    if isinstance(value, str):\n        level = logging.getLevelName(value.upper())\n        if isinstance(level, int):\n            return level\n    elif isinstance(value, int):\n        return value\n\n    return fallback\n\n\ndef setup_logging(\n    default_level: int = DEFAULT_LOG_LEVEL,\n    env_var: str = ENV_VAR_LOG_LEVEL,\n    force: bool = False,\n) -> None:\n    \"\"\"Configure root logging once for scripts and notebooks.\"\"\"\n    global _LOGGER_CONFIGURED\n\n    if _LOGGER_CONFIGURED and not force:\n        return\n\n    env_level = os.getenv(env_var)\n    level = _normalize_level(env_level, default_level)\n\n    root_logger = logging.getLogger()\n\n    if force:\n        for handler in root_logger.handlers[:]:\n            root_logger.removeHandler(handler)\n\n    if not root_logger.handlers:\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(logging.Formatter(DEFAULT_LOG_FORMAT, DEFAULT_DATE_FORMAT))\n        root_logger.addHandler(handler)\n\n    root_logger.setLevel(level)\n    _LOGGER_CONFIGURED = True\n\n\ndef get_logger(name: Optional[str] = None, level: Optional[int] = None) -> logging.Logger:\n    \"\"\"Return a configured logger for the given name.\"\"\"\n    setup_logging()\n    logger = logging.getLogger(name)\n    if level is not None:\n        logger.setLevel(level)\n    return logger\n\n\ndef log_section(\n    logger: logging.Logger,\n    title: str,\n    level: int = logging.INFO,\n    width: int = 80,\n    pad_char: str = \"=\",\n) -> None:\n    \"\"\"Log a section heading similar to the previous banner prints.\"\"\"\n    border = pad_char * width if pad_char else \"\"\n    if border:\n        logger.log(level, border)\n    logger.log(level, title)\n    if border:\n        logger.log(level, border)\n\n\ndef set_global_level(level: int) -> None:\n    \"\"\"Allow callers to adjust root log level at runtime.\"\"\"\n    setup_logging(force=False)\n    logging.getLogger().setLevel(level)\n\nlogger = get_logger(__name__)",
      "id": "c663b6be-bf62-43d9-8e3d-c03ce11a6900"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Data Config",
      "id": "36884e28-d1f2-4be5-a94c-3f9f992acc6b"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": "@dataclass\nclass DataConfig:\n    n_samples: int = 10000\n    n_features: int = 20\n    random_state: int = 42\n    csv_path: Path = Path(\"synthetic_data.csv\")\n    upload_to_snowflake: bool = True\n    connection_name: str = \"legalzoom\"\n    database: str = \"ML_SHOWCASE\"\n    data_schema: str = \"DATA\"\n    table_name: str = \"SYNTHETIC_DATA\"\n\n\n@dataclass\nclass TrainConfig:\n    test_size: float = 0.2\n    random_state: int = 42\n    scaler_path: Path = Path(\"scaler.pkl\")\n    model_path: Path = Path(\"model.pkl\")\n    test_data_path: Path = Path(\"test_data.csv\")\n    metrics_path: Path = Path(\"model_metrics.json\")\n\n\n@dataclass\nclass RegistryConfig:\n    connection_name: str = \"legalzoom\"\n    database: str = \"ML_SHOWCASE\"\n    schema: str = \"MODELS\"\n    model_name: str = \"LINEAR_REGRESSION_CUSTOM\"\n    user_files: Dict[str, list[str]] = field(default_factory=lambda: {\"preprocessing\": [\"scaler.pkl\"]})\n    conda_dependencies: list[str] = field(\n        default_factory=lambda: [\n            \"numpy==1.26.4\",\n            \"pandas==2.1.4\",\n            \"scikit-learn==1.5.2\",\n        ]\n    )\n    pip_requirements: list[str] = field(default_factory=list)\n    artifact_repository_map: Optional[Dict[str, str]] = None\n    resource_constraint: Optional[Dict[str, str]] = None\n    python_version: str = \"3.10\"\n    enable_explainability: bool = False\n    target_platform_mode: str = \"WAREHOUSE_ONLY\"\n\n\n@dataclass\nclass PipelineSteps:\n    generate_data: bool = True\n    train_model: bool = True\n    verify_pickles: bool = True\n    log_model: bool = True\n\n\n@dataclass\nclass ServingConfig:\n    enabled: bool = False\n    compute_pool: Optional[str] = None\n    service_name: Optional[str] = None\n    min_instances: int = 1\n    max_instances: int = 1\n    instance_family: str = \"CPU_X64_M\"\n    force_rebuild: bool = False\n    drop_existing_service: bool = True\n    num_workers: Optional[int] = None\n\n\n@dataclass\nclass PipelineConfig:\n    data: DataConfig = field(default_factory=DataConfig)\n    train: TrainConfig = field(default_factory=TrainConfig)\n    registry: RegistryConfig = field(default_factory=RegistryConfig)\n    steps: PipelineSteps = field(default_factory=PipelineSteps)\n    serving: ServingConfig = field(default_factory=ServingConfig)",
      "id": "1e4db8a3-de44-4ee2-9510-c62127b1f885"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "# High Level Notebook Variables",
      "id": "207880f3-6580-4229-81d1-b07da9105dd2"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": "base_cfg = {\n    \"data\": {\n        # dataset generation\n        \"n_samples\": 5000,\n        \"n_features\": 20,\n        \"random_state\": 42,\n        \"csv_path\": \"notebook_synthetic_data.csv\",\n        \"upload_to_snowflake\": True,  # flip to True when ready\n        \"connection_name\": \"legalzoom\",\n        \"database\": \"ML_SHOWCASE\",\n        \"data_schema\": \"DATA\",\n        \"table_name\": \"SYNTHETIC_DATA\",\n    },\n    \"train\": {\n        \"test_size\": 0.2,\n        \"random_state\": 42,\n        \"scaler_path\": \"scaler.pkl\",\n        \"model_path\": \"model.pkl\",\n        \"test_data_path\": \"test_data.csv\",\n        \"metrics_path\": \"model_metrics.json\",\n    },\n    \"registry\": {\n        \"connection_name\": \"ml_registry\",\n        \"database\": \"ML_SHOWCASE\",\n        \"schema\": \"MODELS\",\n        \"model_name\": \"LINEAR_REGRESSION_CUSTOM\",\n        \"target_platform_mode\": \"SNOWPARK_CONTAINER_SERVICES_ONLY\",\n        \"conda_dependencies\": [],\n        \"pip_requirements\": [\n            \"numpy==1.26.4\",\n            \"pandas==2.1.4\",\n            \"scikit-learn==1.5.2\",\n        ],\n        # Provide these when targeting warehouses to satisfy pip installations:\n        # \"artifact_repository_map\": {\"pip\": \"snowflake.snowpark.pypi_shared_repository\"},\n        # \"resource_constraint\": {\"class\": \"STANDARD_GEN_1\"},\n    },\n    \"steps\": {\n        \"generate_data\": True,\n        \"train_model\": True,\n        \"verify_pickles\": True,\n        \"log_model\": False,  # set True once you're satisfied with the run\n    },\n    \"serving\": {\n        \"enabled\": False,\n        \"compute_pool\": \"ML_INFERENCE_POOL\",\n        \"service_name\": \"LINEAR_REGRESSION_SERVICE\",\n        \"min_instances\": 1,\n        \"max_instances\": 1,\n        \"instance_family\": \"CPU_X64_S\",\n        \"force_rebuild\": True,\n        \"drop_existing_service\": True,\n    },\n}",
      "id": "c5f3fea6-c862-4a33-8de0-42339ae10079"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": "def pipeline_config_from_mapping(mapping: Dict[str, Any]) -> PipelineConfig:\n    \"\"\"Create a pipeline configuration from a nested mapping.\"\"\"\n    cfg = PipelineConfig()\n\n    def _apply(target: Any, values: Dict[str, Any]) -> None:\n        for key, value in values.items():\n            if hasattr(target, key):\n                current = getattr(target, key)\n                if isinstance(current, Path) and not isinstance(value, Path):\n                    setattr(target, key, Path(value))\n                else:\n                    setattr(target, key, value)\n\n    if \"data\" in mapping:\n        _apply(cfg.data, mapping[\"data\"])\n    if \"train\" in mapping:\n        _apply(cfg.train, mapping[\"train\"])\n    if \"registry\" in mapping:\n        _apply(cfg.registry, mapping[\"registry\"])\n    if \"steps\" in mapping:\n        _apply(cfg.steps, mapping[\"steps\"])\n    if \"serving\" in mapping:\n        _apply(cfg.serving, mapping[\"serving\"])\n\n    return cfg",
      "id": "c575dcaf-d70c-47f9-b85e-ce5b915d4ccd"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": "pipeline_cfg = pipeline_config_from_mapping(base_cfg)\npipeline_cfg",
      "id": "1d9e917e-aad3-4eac-bb21-5bcb0b117b7e"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": "try:\n    connection = SnowflakeConnection.from_snow_cli(pipeline_cfg.data.connection_name)\n    session = connection.session\n    print(\"Successfully connected via Snow CLI.\")\nexcept Exception as e:\n    # This will catch any error, including ConfigurationError or ImportError,\n    # and execute the fallback logic.\n    print(f\"Snow CLI connection failed ({type(e).__name__}: {e}). Falling back to get_active_session().\")\n    from snowflake.snowpark.context import get_active_session\n    session = get_active_session()",
      "id": "40c78624-2fb0-46a5-8c60-69be5ef48a94"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Generate Data Set",
      "id": "8f58169e-e193-4bc0-bcc8-c2ed79cdfb75"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": "def generate_synthetic_data(config: DataConfig) -> pd.DataFrame:\n    \"\"\"Generate synthetic regression data.\"\"\"\n    log_section(logger, \"GENERATING SYNTHETIC DATASET\")\n\n    from sklearn.datasets import make_regression\n\n    X, y = make_regression(\n        n_samples=config.n_samples,\n        n_features=config.n_features,\n        n_informative=min(config.n_features, 15),\n        n_targets=1,\n        noise=10.0,\n        bias=50.0,\n        random_state=config.random_state,\n    )\n\n    feature_names = [f\"FEATURE_{i:02d}\" for i in range(config.n_features)]\n    df = pd.DataFrame(X, columns=feature_names)\n    df[\"TARGET\"] = y\n    df.insert(0, \"ID\", range(1, len(df) + 1))\n\n    logger.info(\"Dataset summary: samples=%s, features=%s\", f\"{config.n_samples:,}\", config.n_features)\n    logger.info(\"Target mean=%.2f std=%.2f\", df[\"TARGET\"].mean(), df[\"TARGET\"].std())\n    return df\n\ndef save_to_csv(df: pd.DataFrame, path: Path) -> Path:\n    \"\"\"Persist dataframe to CSV.\"\"\"\n    log_section(logger, \"SAVING DATA TO CSV\")\n    df.to_csv(path, index=False)\n    logger.info(\"Saved data to %s (%.2f MB)\", path, path.stat().st_size / (1024 * 1024))\n    return path\n\ndef upload_to_snowflake(df: pd.DataFrame, config: DataConfig) -> Optional[str]:\n    \"\"\"Upload dataframe to Snowflake if requested.\"\"\"\n    if not config.upload_to_snowflake:\n        logger.info(\"Snowflake upload skipped (upload_to_snowflake=False)\")\n        return None\n\n    log_section(logger, \"UPLOADING DATA TO SNOWFLAKE\")\n    try:\n        connection = SnowflakeConnection.from_snow_cli(pipeline_cfg.data.connection_name)\n        session = connection.session\n        print(\"Successfully connected via Snow CLI.\")\n    except Exception as e:\n        # This will catch any error, including ConfigurationError or ImportError,\n        # and execute the fallback logic.\n        print(f\"Snow CLI connection failed ({type(e).__name__}: {e}). Falling back to get_active_session().\")\n        from snowflake.snowpark.context import get_active_session\n        session = get_active_session()\n    session.sql(f\"CREATE DATABASE IF NOT EXISTS {config.database}\").collect()\n    session.sql(f\"CREATE SCHEMA IF NOT EXISTS {config.data_schema}\").collect()\n    session.sql(f\"USE DATABASE {config.database}\").collect()\n    session.sql(f\"USE SCHEMA {config.data_schema}\").collect()\n\n    session.create_dataframe(df).write.mode(\"overwrite\").save_as_table(config.table_name)\n    logger.info(\n        \"Uploaded data to %s.%s.%s\", config.database, config.data_schema, config.table_name\n    )\n    return f\"{config.database}.{config.data_schema}.{config.table_name}\"",
      "id": "9bfc971a-79dc-4edd-aa9a-c47d24fb6439"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": "print(pipeline_cfg.data)",
      "id": "bdbf71b0-d900-45ca-97b6-2429bb3f2f5e"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": "outputs = {}\n\ndf = generate_synthetic_data(pipeline_cfg.data)\ncsv_path = save_to_csv(df, pipeline_cfg.data.csv_path)\ntable_name = upload_to_snowflake(df, pipeline_cfg.data)\n\noutputs[\"dataframe\"] = df\noutputs[\"csv_path\"] = csv_path\noutputs[\"table_name\"] = table_name",
      "id": "87cbfe09-2192-4c28-9836-7d91f4e7ef4f"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "session.sql(\"SELECT * FROM ML_SHOWCASE.DATA.SYNTHETIC_DATA LIMIT 5\").toPandas()",
      "id": "167a2607-ecaa-4480-a0c9-291443f396e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Custom Z-Scaler & Training and Split",
      "id": "dc4c7796-2bda-4153-bd95-28a3a4f18f3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## Functions",
      "id": "dd0e6911-4b8e-48a0-8b80-56bc426a21c0"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": "import logging\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom typing import Dict, Any\nimport pickle\nimport os",
      "id": "4e0190f3-6209-401c-b9a7-4d7bf57937f3"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": "def split_training_data(\n    df: pd.DataFrame, config: TrainConfig\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n    \"\"\"Split dataset into train/test folds.\"\"\"\n    features = [col for col in df.columns if col.startswith(\"FEATURE_\")]\n    X = df[features]\n    y = df[\"TARGET\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        test_size=config.test_size,\n        random_state=config.random_state,\n    )\n    logger.info(\n        \"Split data: train=%s, test=%s (test_size=%.0f%%)\",\n        f\"{len(X_train):,}\",\n        f\"{len(X_test):,}\",\n        config.test_size * 100,\n    )\n    return X_train, X_test, y_train, y_test",
      "id": "9b1ecda8-f787-4beb-8535-733ddf915671"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": "from custom_model import CustomZScaler",
      "id": "14b1d6c0-446f-473e-bc13-e043bceb9a14"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": "\ndef train_model_with_preprocessing(X_train: pd.DataFrame, y_train: pd.Series):\n    \"\"\"\n    Train model with preprocessing and save separate pickle files.\n    \n    This function:\n    1. Creates and fits the custom Z-scaler\n    2. Transforms training data\n    3. Trains linear regression model\n    4. Saves scaler and model as separate pickle files\n    \n    Parameters:\n    -----------\n    X_train : pd.DataFrame\n        Training features\n    y_train : pd.Series or np.ndarray\n        Training targets\n        \n    Returns:\n    --------\n    scaler : CustomZScaler\n        Fitted scaler\n    model : LinearRegression\n        Fitted model\n    \"\"\"\n    log_section(logger, \"TRAINING MODEL WITH PREPROCESSING\")\n    \n    # Step 1: Create and fit the custom Z-scaler\n    logger.info(\"1. Fitting Custom Z-Scaler...\")\n    scaler = CustomZScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    \n    logger.info(\"   Scaler fitted\")\n    logger.info(\"   Features: %s\", len(scaler.feature_names_))\n    logger.info(\n        \"   Mean range: [%.2f, %.2f]\",\n        scaler.mean_.min(),\n        scaler.mean_.max(),\n    )\n    logger.info(\n        \"   Std range: [%.2f, %.2f]\",\n        scaler.std_.min(),\n        scaler.std_.max(),\n    )\n    \n    # Step 2: Train linear regression model\n    logger.info(\"2. Training Linear Regression Model...\")\n    model = LinearRegression()\n    model.fit(X_train_scaled, y_train)\n    \n    logger.info(\"   Model trained\")\n    logger.info(\"   Coefficients: %s\", len(model.coef_))\n    logger.info(\"   Intercept: %.2f\", model.intercept_)\n    \n    # Step 3: Save as separate pickle files\n    logger.info(\"3. Saving Components as Separate Pickle Files...\")\n    \n    scaler_path = \"scaler.pkl\"\n    model_path = \"model.pkl\"\n    \n    with open(scaler_path, \"wb\") as f:\n        pickle.dump(scaler, f)\n    logger.info(\"   Saved scaler to: %s\", scaler_path)\n    \n    with open(model_path, \"wb\") as f:\n        pickle.dump(model, f)\n    logger.info(\"   Saved model to: %s\", model_path)\n    \n    logger.info(\"Training complete!\")\n    logger.info(\"  Components saved as separate pickle files:\")\n    logger.info(\"    - %s (preprocessing)\", scaler_path)\n    logger.info(\"    - %s (model)\", model_path)\n    \n    return scaler, model",
      "id": "a1f7ab62-97a8-4b34-b00a-098d132cf4a5"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": "def evaluate_model(\n    scaler,\n    model,\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n) -> Dict[str, Dict[str, float]]:\n    \"\"\"Compute evaluation metrics for train/test sets.\"\"\"\n    log_section(logger, \"EVALUATING MODEL\")\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    train_pred = model.predict(X_train_scaled)\n    test_pred = model.predict(X_test_scaled)\n\n    metrics = {\n        \"train\": {\n            \"mse\": mean_squared_error(y_train, train_pred),\n            \"rmse\": np.sqrt(mean_squared_error(y_train, train_pred)),\n            \"mae\": mean_absolute_error(y_train, train_pred),\n            \"r2\": r2_score(y_train, train_pred),\n        },\n        \"test\": {\n            \"mse\": mean_squared_error(y_test, test_pred),\n            \"rmse\": np.sqrt(mean_squared_error(y_test, test_pred)),\n            \"mae\": mean_absolute_error(y_test, test_pred),\n            \"r2\": r2_score(y_test, test_pred),\n        },\n    }\n\n    for split in (\"train\", \"test\"):\n        logger.info(\n            \"%s metrics: RMSE=%.4f MAE=%.4f R2=%.4f\",\n            split.capitalize(),\n            metrics[split][\"rmse\"],\n            metrics[split][\"mae\"],\n            metrics[split][\"r2\"],\n        )\n    return metrics",
      "id": "bcece142-6da1-4b32-bd70-d5c15731a68b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## Train Model",
      "id": "62176de3-5b8d-430e-8905-2077c013a77a"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": "pipeline_cfg.train",
      "id": "febe2cf9-ce95-4038-8868-c667c478ccc3"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": "X_train, X_test, y_train, y_test = split_training_data(df, pipeline_cfg.train)\nscaler, model = train_model_with_preprocessing(X_train, y_train)",
      "id": "1cff010c-a7c1-4ad5-a45a-e2879810c328"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": "! ls | grep pkl",
      "id": "0c04afc8-910e-4284-abb7-f52a9ff8fb72"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": "metrics = evaluate_model(scaler, model, X_train, X_test, y_train, y_test)\nmetrics",
      "id": "ee07df21-cc74-4359-b61c-dd8c9214e5f4"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": "from snowflake.ml.model import custom_model\n\nmodel_ctx = custom_model.ModelContext(\n    model_path=str(pipeline_cfg.train.model_path.resolve()),\n    scaler_path=str(pipeline_cfg.train.scaler_path.resolve()),\n)\n\nimport importlib\nimport types\n\ndef _ensure_custom_module_alias() -> None:\n    if \"custom_model\" in sys.modules:\n        sys.modules.setdefault(\"model_registry_showcase.custom_model\", sys.modules[\"custom_model\"])\n        return\n    module = None\n    try:\n        module = importlib.import_module(\"custom_model\")\n    except ModuleNotFoundError:\n        try:\n            module = importlib.import_module(\"model_registry_showcase.custom_model\")\n        except ModuleNotFoundError:\n            module = _create_embedded_custom_module()\n    sys.modules[\"custom_model\"] = module\n    sys.modules.setdefault(\"model_registry_showcase.custom_model\", module)\n\ndef _create_embedded_custom_module() -> types.ModuleType:\n    module = types.ModuleType(\"custom_model\")\n    module.CustomZScaler = CustomZScaler\n    return module\n\nclass RegressionWithScaler(custom_model.CustomModel):\n    def __init__(self, context):\n        super().__init__(context)\n        _ensure_custom_module_alias()\n        with Path(self.context[\"model_path\"]).open(\"rb\") as f:\n            self.model = pickle.load(f)\n        with Path(self.context[\"scaler_path\"]).open(\"rb\") as f:\n            self.scaler = pickle.load(f)\n\n    @custom_model.inference_api\n    def predict(self, input: pd.DataFrame) -> pd.DataFrame:\n        feature_cols = [col for col in input.columns if col.startswith(\"FEATURE_\")]\n        X_scaled = self.scaler.transform(input[feature_cols])\n        y_hat = self.model.predict(X_scaled)\n        return pd.DataFrame({\"PREDICTION\": y_hat}, index=input.index)\n\n\ncustom_model_obj = RegressionWithScaler(model_ctx)",
      "id": "6621b0e5-3857-496b-b389-abe229ddc7bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Model Registry",
      "id": "51ef7099-b5a9-4999-85a2-38ff188c9e3c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_pickle(path: Path):\n    \"\"\"Utility to load a pickle file.\"\"\"\n    with path.open(\"rb\") as fh:\n        return pickle.load(fh)",
      "id": "6b6c09fe-87d4-41b1-bdd6-2f64578e3c93"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "log_section(logger, \"INITIALIZING SNOWFLAKE REGISTRY\")\nregistry = Registry(\n    session=session,\n    database_name=pipeline_cfg.registry.database,\n    schema_name=pipeline_cfg.registry.schema,\n)\nlogger.info(\"Registry ready at %s\", registry.location)",
      "id": "46782ff8-892a-48f4-95b3-8a0a74f2013b"
    },
    {
      "id": "1d07c37e-5580-488f-89a5-3bb128c9e6c2",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "pipeline_cfg.train.test_data_path",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "sample_df = df[:10]\nfeature_cols = [col for col in sample_df.columns if col.startswith(\"FEATURE_\")]\nsample_data = sample_df[feature_cols].head(5)\nsample_data",
      "id": "22f58c75-3071-487a-9053-694acabf6493"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "log_section(logger, \"LOGGING MODEL VERSION\")\nmodel = load_pickle(Path(\"model.pkl\"))\nscaler = load_pickle(Path(\"scaler.pkl\"))",
      "id": "7211ef03-0674-428e-bbc8-b3d7b14681cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "for subdir, files in pipeline_cfg.registry.user_files.items():\n    for file_name in files:\n        path = Path(file_name)\n        if not path.exists():\n            raise FileNotFoundError(f\"user_files entry not found: {path}\")\n        if path.stat().st_size > 5 * 1024 * 1024 * 1024:\n            raise ValueError(f\"user_files entry exceeds 5GB limit: {path}\")\n\nsample_scaled = pd.DataFrame(\n    scaler.transform(sample_data),\n    columns=sample_data.columns,\n)\nversion_name = f\"v_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\nlogger.info(\"Logging model %s version %s\", pipeline_cfg.registry.model_name, version_name)",
      "id": "b7cc72f5-33a7-4644-bb25-57f029246237"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "mode_token = (pipeline_cfg.registry.target_platform_mode or \"\").upper()\nruns_in_warehouse = \"WAREHOUSE\" in mode_token if mode_token else True\n\nimport importlib, sys\ncustom_model_mod = importlib.import_module(\"custom_model\")\nsys.modules.setdefault(\"custom_model\", custom_model_mod)\nsys.modules.setdefault(\"model_registry_showcase.custom_model\", custom_model_mod)\n\noptions = {\n    \"enable_explainability\": pipeline_cfg.registry.enable_explainability,\n    \"target_methods\": [\"predict\"],\n}\nif not pipeline_cfg.registry.pip_requirements and not pipeline_cfg.registry.conda_dependencies:\n    options[\"relax_version\"] = True\n\nlog_kwargs = dict(\n    model=custom_model_obj,\n    model_name=pipeline_cfg.registry.model_name,\n    version_name=version_name,\n    comment=(\n        \"Linear Regression with custom preprocessing. \"\n        f\"Logged on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n        \".\"\n    ),\n    metrics=metrics,\n    python_version=pipeline_cfg.registry.python_version,\n    sample_input_data=sample_scaled,\n    options=options,\n    ext_modules=[\n        sys.modules[RegressionWithScaler.__module__],\n        custom_model_mod,\n    ],\n)\n\nif pipeline_cfg.registry.conda_dependencies:\n    log_kwargs[\"conda_dependencies\"] = pipeline_cfg.registry.conda_dependencies\n\nif pipeline_cfg.registry.pip_requirements:\n    log_kwargs[\"pip_requirements\"] = pipeline_cfg.registry.pip_requirements\n    log_kwargs[\"options\"].pop(\"relax_version\", None)\n\nartifact_map = pipeline_cfg.registry.artifact_repository_map\nif artifact_map and not runs_in_warehouse:\n    logger.debug(\n        \"Ignoring artifact_repository_map because target platform mode %s does not include WAREHOUSE.\",\n        pipeline_cfg.registry.target_platform_mode,\n    )\n    artifact_map = None\nif artifact_map is None and pipeline_cfg.registry.pip_requirements and runs_in_warehouse:\n    artifact_map = {\"pip\": \"snowflake.snowpark.pypi_shared_repository\"}\nif artifact_map:\n    log_kwargs[\"artifact_repository_map\"] = artifact_map\n\nresource_constraint = pipeline_cfg.registry.resource_constraint if runs_in_warehouse else None\nif resource_constraint:\n    log_kwargs[\"resource_constraint\"] = resource_constraint\n\ntarget_platform_arg = None\nif TargetPlatform is not None and pipeline_cfg.registry.target_platform_mode:\n    target_platform_arg = getattr(TargetPlatform, pipeline_cfg.registry.target_platform_mode, None)\n    if target_platform_arg is None:\n        logger.warning(\n            \"Unknown target_platform_mode '%s'\", pipeline_cfg.registry.target_platform_mode\n        )\nelif pipeline_cfg.registry.target_platform_mode:\n    target_platform_arg = pipeline_cfg.registry.target_platform_mode\n\nif target_platform_arg:\n    log_kwargs[\"target_platform\"] = target_platform_arg\n\nmodel_version = registry.log_model(**log_kwargs)\nlogger.info(\"Logged model version: %s\", model_version.version_name)",
      "id": "6bf8ab4e-64ca-4383-b690-5ebbd28a00c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "session.sql(\"SHOW MODELS IN DATABASE ML_SHOWCASE\").toPandas()",
      "id": "9497f05d-93e4-4bf7-bec8-6554f5b3bec6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from snowflake.ml.registry import registry\n\ntry:\n    reg = registry.Registry(session=session, database_name='ML_SHOWCASE', schema_name='MODELS')\n    mv = reg.get_model('LINEAR_REGRESSION_CUSTOM').version('V_20251110_190603')\n    mv.run(sample_data, function_name='PREDICT') # Need to Deploy a SPCS Endpoint\nexcept Exception as e:\n    print(e)\n    print(\"Need to Deploy a SPCS Endpoint\")",
      "id": "60c6c0ca-f15b-4d55-83bb-a177ff4707ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Deploy Service ",
      "id": "4d4dec27-d517-432f-a1ee-f39172566d27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def ensure_compute_pool(session, serving_cfg: ServingConfig) -> None:\n    if not serving_cfg.compute_pool:\n        return\n\n    create_sql = f\"\"\"\n    CREATE COMPUTE POOL IF NOT EXISTS {serving_cfg.compute_pool}\n        MIN_NODES = {serving_cfg.min_instances}\n        MAX_NODES = {serving_cfg.max_instances}\n        INSTANCE_FAMILY = '{serving_cfg.instance_family}'\n    \"\"\"\n    session.sql(create_sql).collect()\n    logger.info(\"Compute pool ensured: %s\", serving_cfg.compute_pool)",
      "id": "209c0324-b035-4ab6-945e-4f6bb2c76f4b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from snowflake.ml.registry import registry\nregistry = registry.Registry(session=session, database_name='ML_SHOWCASE', schema_name='MODELS')\nensure_compute_pool(session,  pipeline_cfg.serving)",
      "id": "c7676a44-a2d6-4575-b424-abf3abddd082"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "deploy_kwargs: Dict[str, Any] = {}\ncompute_pool = pipeline_cfg.serving.compute_pool\nif compute_pool:\n    deploy_kwargs[\"compute_pool\"] = compute_pool\n\nbase_service_name = (\n    pipeline_cfg.serving.service_name\n    or f\"{model_version.model_name}_{model_version.version_name}\"\n)\nif \".\" in base_service_name:\n    qualified_service_name = base_service_name\nelse:\n    qualified_service_name = (\n        f\"{pipeline_cfg.registry.database}.{pipeline_cfg.registry.schema}.{base_service_name}\"\n    )\n\ndeploy_kwargs[\"service_name\"] = qualified_service_name\nif hasattr(pipeline_cfg.serving, \"min_instances\"):\n    deploy_kwargs[\"min_instances\"] = pipeline_cfg.serving.min_instances\nif hasattr(pipeline_cfg.serving, \"max_instances\"):\n    deploy_kwargs[\"max_instances\"] = pipeline_cfg.serving.max_instances\nif getattr(pipeline_cfg.serving, \"force_rebuild\", False):\n    deploy_kwargs[\"force_rebuild\"] = pipeline_cfg.serving.force_rebuild\nif getattr(pipeline_cfg.serving, \"num_workers\", None) is not None:\n    deploy_kwargs[\"num_workers\"] = pipeline_cfg.serving.num_workers\n\nif pipeline_cfg.serving.drop_existing_service:\n    try:\n        session.sql(f\"DROP SERVICE IF EXISTS {qualified_service_name}\").collect()\n        logger.info(\"Dropped existing service if it existed: %s\", qualified_service_name)\n    except Exception as exc:\n        logger.debug(\"Unable to drop existing service %s: %s\", qualified_service_name, exc)\n\nservice = None\nif hasattr(model_version, \"deploy_to_snowpark_container_services\"):\n    service = model_version.deploy_to_snowpark_container_services(**deploy_kwargs)\nelif hasattr(model_version, \"deploy\"):\n    deploy_kwargs.setdefault(\"target_platform\", \"SNOWPARK_CONTAINER_SERVICES\")\n    service = model_version.deploy(**deploy_kwargs)\nelif hasattr(model_version, \"create_service\"):\n    if not compute_pool:\n        raise ValueError(\"serving.compute_pool must be set to use create_service().\")\n    create_kwargs: Dict[str, Any] = {\n        \"service_name\": qualified_service_name,\n        \"service_compute_pool\": compute_pool,\n        \"ingress_enabled\": True,\n        \"max_instances\": pipeline_cfg.serving.max_instances,\n    }\n    if getattr(pipeline_cfg.serving, \"force_rebuild\", False):\n        create_kwargs[\"force_rebuild\"] = pipeline_cfg.serving.force_rebuild\n    if getattr(pipeline_cfg.serving, \"num_workers\", None) is not None:\n        create_kwargs[\"num_workers\"] = pipeline_cfg.serving.num_workers\n    service = model_version.create_service(**create_kwargs)\nelse:\n    raise AttributeError(\"Snowflake ML SDK does not expose an SPCS deployment helper in this version.\")\n\nlogger.info(\"Requested service deployment for %s\", qualified_service_name)",
      "id": "114a78cf-203c-436d-95f3-10ca674a41a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from snowflake.ml.registry import Registry\n\n# reuse the same registry/session you used to create the service\nregistry = Registry(session=session, database_name=\"ML_SHOWCASE\", schema_name=\"MODELS\")\nmv = registry.get_model(\"LINEAR_REGRESSION_CUSTOM\").default   # or pick a specific version\n\nmv.show_functions()  # optional: see which inference functions are exposed",
      "id": "efa1feba-f877-49d5-a843-2c432948ed0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "prediction_df = mv.run(\n    sample_data,                      # pandas or Snowpark DataFrame\n    function_name=\"predict\",      # matches @custom_model.inference_api name\n    service_name=\"LINEAR_REGRESSION_SERVICE\"  # the service you created\n)",
      "id": "80f2ff9b-2a50-4ec7-b145-a1a17f2860ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "sample_data",
      "id": "48ffa998-7fd0-4f6a-9ab9-451ab8eea4de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "prediction_df",
      "id": "2780e11c-6aec-4256-959f-8118f8d9c1e3"
    },
    {
      "id": "814d9fa2-cae2-4b85-84ac-c0003ecf8d66",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-registry-showcase",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}