{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Snowflake Model Registry ‚Äî End-to-End Template\n",
        "\n",
        "This notebook is a teaching scaffold: every step is explicit, and you can replace any block with your own code. Work top-to-bottom, editing the sections marked **‚Äúüîß Customize‚Äù** as you go.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Prerequisites\n",
        "\n",
        "1. Activate the project environment (from a terminal):\n",
        "   ```bash\n",
        "   conda activate legalzoom-env\n",
        "   ```\n",
        "2. Ensure your Snowflake CLI connection (e.g. `legalzoom`) is configured with the right credentials.\n",
        "3. Run this notebook from `/Users/jdemlow/github/legal-zoom` so relative imports resolve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../model_registry_showcase\")\n",
        "\n",
        "from logging_utils import get_logger, set_global_level\n",
        "from core import (\n",
        "    pipeline_config_from_mapping,\n",
        "    generate_synthetic_data,\n",
        "    save_to_csv,\n",
        "    upload_to_snowflake,\n",
        "    split_training_data,\n",
        "    evaluate_model,\n",
        "    save_training_artifacts,\n",
        "    verify_pickles,\n",
        "    init_registry,\n",
        "    log_model_version,\n",
        "    deploy_inference_service,\n",
        ")\n",
        "from custom_model import CustomZScaler, train_model_with_preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the shared logging utilities so that output is consistent with the command-line tools.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "logger = get_logger(__name__)\n",
        "set_global_level(\"INFO\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. üîß Customize your configuration\n",
        "\n",
        "Edit the dictionaries below with your own dataset parameters, Snowflake identifiers, and toggles. Everything else in the notebook reads from `cfg`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "base_cfg = {\n",
        "    \"data\": {\n",
        "        # dataset generation\n",
        "        \"n_samples\": 5000,\n",
        "        \"n_features\": 20,\n",
        "        \"random_state\": 42,\n",
        "        \"csv_path\": \"notebook_synthetic_data.csv\",\n",
        "        \"upload_to_snowflake\": False,  # flip to True when ready\n",
        "        \"connection_name\": \"legalzoom\",\n",
        "        \"database\": \"ML_SHOWCASE\",\n",
        "        \"data_schema\": \"DATA\",\n",
        "        \"table_name\": \"SYNTHETIC_DATA\",\n",
        "    },\n",
        "    \"train\": {\n",
        "        \"test_size\": 0.2,\n",
        "        \"random_state\": 42,\n",
        "        \"scaler_path\": \"scaler.pkl\",\n",
        "        \"model_path\": \"model.pkl\",\n",
        "        \"test_data_path\": \"test_data.csv\",\n",
        "        \"metrics_path\": \"model_metrics.json\",\n",
        "    },\n",
        "    \"registry\": {\n",
        "        \"connection_name\": \"legalzoom\",\n",
        "        \"database\": \"ML_SHOWCASE\",\n",
        "        \"schema\": \"MODELS\",\n",
        "        \"model_name\": \"LINEAR_REGRESSION_CUSTOM\",\n",
        "        \"target_platform_mode\": \"WAREHOUSE_ONLY\",  # or SNOWPARK_CONTAINER_SERVICES_ONLY\n",
        "    },\n",
        "    \"steps\": {\n",
        "        \"generate_data\": True,\n",
        "        \"train_model\": True,\n",
        "        \"verify_pickles\": True,\n",
        "        \"log_model\": False,  # set True once you're satisfied with the run\n",
        "    },\n",
        "    \"serving\": {\n",
        "        \"enabled\": False,\n",
        "        \"compute_pool\": \"ML_INFERENCE_POOL\",\n",
        "        \"service_name\": \"LINEAR_REGRESSION_SERVICE\",\n",
        "        \"min_instances\": 1,\n",
        "        \"max_instances\": 1,\n",
        "        \"instance_family\": \"CPU_X64_M\",\n",
        "    },\n",
        "}\n",
        "\n",
        "cfg = pipeline_config_from_mapping(base_cfg)\n",
        "cfg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. üîß Customize the dataset builder\n",
        "\n",
        "Feel free to swap in your own data-loading logic. The helper below defaults to `sklearn.datasets.make_regression`, but you can replace it with SQL pulls, CSV loads, or feature engineering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def build_dataset(config):\n",
        "    \"\"\"Return a pandas DataFrame with feature columns + TARGET.\n",
        "\n",
        "    Replace this function with your own data ingestion if desired.\n",
        "    \"\"\"\n",
        "    df = generate_synthetic_data(config.data)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "df = build_dataset(cfg)\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate/save artifacts (local)\n",
        "\n",
        "This step always saves a CSV so you can inspect the raw features. Upload to Snowflake only when you set `upload_to_snowflake=True` above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "csv_path = save_to_csv(df, cfg.data.csv_path)\n",
        "logger.info(\"Saved local dataset to %s\", csv_path)\n",
        "\n",
        "if cfg.data.upload_to_snowflake:\n",
        "    table_name = upload_to_snowflake(df, cfg.data)\n",
        "    logger.info(\"Uploaded dataset to %s\", table_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. üîß Customize preprocessing/modeling\n",
        "\n",
        "`CustomZScaler` is provided out of the box. If you want to add feature engineering, try editing the cell below (or swap in your own transformer/model entirely).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = split_training_data(df, cfg.train)\n",
        "scaler, model = train_model_with_preprocessing(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "metrics = evaluate_model(scaler, model, X_train, X_test, y_train, y_test)\n",
        "metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Persist artifacts & verify pickles\n",
        "\n",
        "This mirrors the CLI flow: write test data + metrics, pickle the scaler/model, then double-check they play nicely together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "save_training_artifacts(X_test, y_test, metrics, cfg.train)\n",
        "\n",
        "verified = verify_pickles(cfg.train.scaler_path, cfg.train.model_path, cfg.train.test_data_path)\n",
        "logger.info(\"Pickle verification passed? %s\", verified)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. üîß Log to Snowflake (toggle when ready)\n",
        "\n",
        "Set `cfg.steps.log_model = True` and re-run this cell to push the model into the registry. Make sure `upload_to_snowflake=True` earlier so the dataset is staged in your account.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "if cfg.steps.log_model:\n",
        "    connection, registry = init_registry(cfg.registry)\n",
        "    sample_df = pd.read_csv(cfg.train.test_data_path)\n",
        "    feature_cols = [col for col in sample_df.columns if col.startswith(\"FEATURE_\")]\n",
        "    sample_data = sample_df[feature_cols].head(5)\n",
        "    try:\n",
        "        model_version = log_model_version(registry, cfg.registry, sample_data, metrics)\n",
        "        logger.info(\"Logged Snowflake model version: %s\", model_version.version_name)\n",
        "    finally:\n",
        "        connection.close()\n",
        "else:\n",
        "    logger.info(\"Model logging skipped (set cfg.steps.log_model = True to enable)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Optional: deploy to Snowpark Container Services\n",
        "\n",
        "Fill in the compute pool details above (`cfg.serving.enabled = True`) and run the cell when you're ready to request a managed service.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "if cfg.serving.enabled:\n",
        "    service = deploy_inference_service(cfg.registry, cfg.serving)\n",
        "    logger.info(\"Snowpark Container Services deployment initiated: %s\", service)\n",
        "else:\n",
        "    logger.info(\"SPCS deployment skipped (set cfg.serving.enabled = True to enable)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Where to go next\n",
        "- Swap out the dataset builder for your own feature pipeline.\n",
        "- Embed additional preprocessing inside `train_model_with_preprocessing` or replace it entirely.\n",
        "- Turn on the Snowflake/serving flags and monitor the CLI (`run_pipeline.py`) to compare outputs.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
